"""LLM Provider for weather agent"""

import os
from typing import List, Dict, Any
from llm_prompts import (
    PLANNING_PROMPT, TOOL_SELECTION_PROMPT, RESPONSE_PROMPT, 
    LEARNING_PROMPT, SYSTEM_MESSAGES
)


class LLMProvider:
    """LLM provider for all operations"""
    
    def __init__(self, provider: str = "auto"):
        self.provider = self._detect_provider(provider)
        self.client = self._initialize_client()
    
    def _detect_provider(self, provider: str) -> str:
        """Auto-detect best available provider"""
        if provider == "auto":
            # Priority order: Groq (faster), then OpenAI
            if self._is_groq_available():
                return "groq"
            elif self._is_openai_available():
                return "openai"
            else:
                raise ValueError("No LLM provider available. Set GROQ_API_KEY or OPENAI_API_KEY in .env")
        return provider
    
    def _is_groq_available(self) -> bool:
        """Check if Groq is available"""
        try:
            import groq
            return bool(os.getenv("GROQ_API_KEY"))
        except ImportError:
            return False
    
    def _is_openai_available(self) -> bool:
        """Check if OpenAI is available"""
        try:
            import openai
            return bool(os.getenv("OPENAI_API_KEY"))
        except ImportError:
            return False
    
    def _initialize_client(self):
        """Initialize the appropriate LLM client"""
        if self.provider == "groq":
            import groq
            if not os.getenv("GROQ_API_KEY"):
                raise ValueError("GROQ_API_KEY not set in .env file")
            return groq.Groq(api_key=os.getenv("GROQ_API_KEY"))
        
        elif self.provider == "openai":
            import openai
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY not set in .env file")
            return openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")
    
    def _generate(self, messages: List[Dict[str, str]], temperature: float = 0.1, max_tokens: int = 800) -> str:
        """Generate response using the configured provider"""
        
        if self.provider == "groq":
            model = "llama-3.1-8b-instant"
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        
        elif self.provider == "openai":
            model = "gpt-3.5-turbo"
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content.strip()
        
        else:
            raise ValueError(f"Unsupported provider: {self.provider}")
    
    def plan(self, query: str) -> str:
        """Generate execution plan for the query"""
        messages = [
            {"role": "system", "content": SYSTEM_MESSAGES["planner"]},
            {"role": "user", "content": PLANNING_PROMPT.format(query=query)}
        ]
        
        plan = self._generate(messages, temperature=0.1, max_tokens=800)
        return f"{plan}\n\n**ðŸ¤– Note:** This reasoning was generated by {self.get_provider_info()}."
    
    def select_tools(self, query: str) -> List[str]:
        """Select appropriate tools for the query"""
        messages = [
            {"role": "system", "content": SYSTEM_MESSAGES["tool_selector"]},
            {"role": "user", "content": TOOL_SELECTION_PROMPT.format(query=query)}
        ]
        
        tool_response = self._generate(messages, temperature=0.1, max_tokens=100)
        
        # Parse the response and ensure we have valid tools
        selected_tools = [tool.strip() for tool in tool_response.split(",")]
        
        # Ensure we always have basic tools
        if "weather" not in selected_tools:
            selected_tools.append("weather")
        if "rag" not in selected_tools:
            selected_tools.append("rag")
        
        return selected_tools
    
    def respond(self, context: Dict[str, Any]) -> str:
        """Generate response based on execution context"""
        messages = [
            {"role": "system", "content": SYSTEM_MESSAGES["responder"]},
            {"role": "user", "content": RESPONSE_PROMPT.format(**context)}
        ]
        
        response = self._generate(messages, temperature=0.3, max_tokens=1200)
        return f"{response}\n\n**ðŸ¤– Note:** This response was generated by {self.get_provider_info()} analysis of execution results."
    
    def learn(self, interaction: Dict[str, Any]) -> str:
        """Generate learning insights from interaction"""
        messages = [
            {"role": "system", "content": SYSTEM_MESSAGES["learner"]},
            {"role": "user", "content": LEARNING_PROMPT.format(**interaction)}
        ]
        
        return self._generate(messages, temperature=0.2, max_tokens=200)
    
    def get_provider_info(self) -> str:
        """Get information about the current provider"""
        if self.provider == "groq":
            return "Groq (Llama 3.1-8b)"
        elif self.provider == "openai":
            return "OpenAI (GPT-3.5-turbo)"
        else:
            return f"Unknown provider: {self.provider}"
