{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf43366",
   "metadata": {},
   "source": [
    "Build a basic chatbot (langraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6827f",
   "metadata": {},
   "source": [
    "1. Load enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262db1d",
   "metadata": {},
   "source": [
    "Imports and State Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import SecretStr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f59fb0",
   "metadata": {},
   "source": [
    "2. Create State and define variable for the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List, add_messages]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11979a2",
   "metadata": {},
   "source": [
    "3. Initialize the LLM Model [groq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151672c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    api_key=SecretStr(os.getenv(\"GROQ_API_KEY\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c64784",
   "metadata": {},
   "source": [
    "4. Create chatbot method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    # The LLM expects a list of messages\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "def message_builder(role, message):\n",
    "    return {\"messages\": [{\"role\": role, \"content\": message}]}\n",
    "\n",
    "def invoke_message(graph, message, role, config=None):\n",
    "    item=message_builder(role, message)\n",
    "    if not config:\n",
    "        return graph.invoke(item)\n",
    "    return graph.invoke(item, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b98ae",
   "metadata": {},
   "source": [
    "5. Build Graph with node and edges [start -> chatbot -> end] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf027ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "#add node\n",
    "graph_builder.add_node(\"llm_chatbot\", chatbot)\n",
    "\n",
    "#add edge\n",
    "graph_builder.add_edge(START, \"llm_chatbot\")\n",
    "graph_builder.add_edge(\"llm_chatbot\", END)\n",
    "\n",
    "#compile graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56249da2",
   "metadata": {},
   "source": [
    "Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize graph\n",
    "from IPython.display import Image, display\n",
    "def display_graph(graph):\n",
    "    try:\n",
    "        display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc63c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_message(graph, \"Hello, how are you?\", \"user\")\n",
    "#graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb37dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"messages\"][-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in graph.stream({\"messages\":[{\"role\": \"user\", \"content\": \"Hello, how do you you?\"}]}):\n",
    "    for value in event.values():\n",
    "        print(value[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976ee96",
   "metadata": {},
   "source": [
    "# Chatbot with Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_tool=TavilySearch(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom function\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"\n",
    "        Multiply a and b\n",
    "        \n",
    "        Args:\n",
    "            a:int first int\n",
    "            b:int second int\n",
    "        \n",
    "        Returns:\n",
    "            int: output \n",
    "    \"\"\"\n",
    "    return a*b\n",
    "\n",
    "def pretty_print(response):\n",
    "    for message in response['messages']:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[tavily_tool,multiply]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e788eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## State Graph for tools\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "## Node defination\n",
    "def tool_calling_llm(state: State):\n",
    "      return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "## Graph\n",
    "tool_graph_builder = StateGraph(State)\n",
    "tool_graph_builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "tool_graph_builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "##add edges\n",
    "tool_graph_builder.add_edge(START, \"tool_calling_llm\")\n",
    "\n",
    "\n",
    "#add conditional edge\n",
    "tool_graph_builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\", \n",
    "    # if the latest message (result) from assistant is a tool call -> tools_condition routes it to \"tools\"\n",
    "    # if the latest message (result) from assistant is not a tool call -> tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "tool_graph_builder.add_edge(\"tools\", END)\n",
    "\n",
    "#compile the graph\n",
    "graph=tool_graph_builder.compile()\n",
    "display_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response= invoke_message(graph, \"What is the recent AI news?\", \"user\")\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=  invoke_message(graph,\"what is 2 multiply by 3?\" ,\"user\")\n",
    "pretty_print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f0962c",
   "metadata": {},
   "source": [
    "# ReAct agent Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff19b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## State Graph for tools\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "## Node defination\n",
    "def tool_calling_llm(state: State):\n",
    "      return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "## Graph\n",
    "tool_graph_builder = StateGraph(State)\n",
    "tool_graph_builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "tool_graph_builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "##add edges\n",
    "tool_graph_builder.add_edge(START, \"tool_calling_llm\")\n",
    "\n",
    "\n",
    "#add conditional edge\n",
    "tool_graph_builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\", \n",
    "    # if the latest message (result) from assistant is a tool call -> tools_condition routes it to \"tools\"\n",
    "    # if the latest message (result) from assistant is not a tool call -> tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "tool_graph_builder.add_edge(\"tools\", \"tool_calling_llm\") # Agentic nature added\n",
    "\n",
    "\n",
    "#compile the graph\n",
    "graph=tool_graph_builder.compile()\n",
    "display_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response= invoke_message(graph,\"Please update me with recent AI news and than 2 multiply by 3?\" ,\"user\")\n",
    "pretty_print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6cfba",
   "metadata": {},
   "source": [
    "## Adding Memory in Agenting Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ed5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## State Graph for tools\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver # memory for llm \n",
    "\n",
    "## Node defination\n",
    "def tool_calling_llm(state: State):\n",
    "      return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "## Graph\n",
    "tool_graph_builder = StateGraph(State)\n",
    "tool_graph_builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "tool_graph_builder.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "##add edges\n",
    "tool_graph_builder.add_edge(START, \"tool_calling_llm\")\n",
    "\n",
    "\n",
    "#add conditional edge\n",
    "tool_graph_builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\", \n",
    "    # if the latest message (result) from assistant is a tool call -> tools_condition routes it to \"tools\"\n",
    "    # if the latest message (result) from assistant is not a tool call -> tools_condition routes to END\n",
    "    tools_condition\n",
    ")\n",
    "\n",
    "tool_graph_builder.add_edge(\"tools\", \"tool_calling_llm\") # Agentic nature added\n",
    "\n",
    "\n",
    "#compile the graph\n",
    "memory = MemorySaver()\n",
    "graph=tool_graph_builder.compile(checkpointer=memory)\n",
    "display_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ad129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_config(id=1):\n",
    "    return {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37654a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = thread_config() # Example, adjust as needed\n",
    "response = invoke_message(graph,\"My name is Nitin\" ,\"user\", config) \n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42199b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_message(graph,\"What is My name \" ,\"user\", config)  \n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a56a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_message(graph,\"Do you remember me  \" ,\"user\", config) \n",
    "response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3b786",
   "metadata": {},
   "source": [
    "Streaming\n",
    "Methods: .stream() and astream()\n",
    "\n",
    "These methods are sync and async methods for streaming back results.\n",
    "Additional parameters in streaming modes for graph state\n",
    "\n",
    "values : This streams the full state of the graph after each node is called.\n",
    "updates : This streams updates to the state of the graph after each node is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517602bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "def superbot(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state['messages'])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc31149",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "#node\n",
    "graph_builder.add_node(\"Superbot\", superbot )\n",
    "graph_builder.add_edge(START, \"Superbot\")\n",
    "graph_builder.add_edge(\"Superbot\", END)\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b140da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## invocation\n",
    "config = thread_config() # Example, adjust as needed\n",
    "\n",
    "response = invoke_message(graph,\"My name is Nitin and i like video games\" ,\"user\", config) \n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c19fc8",
   "metadata": {},
   "source": [
    "Stream: Update mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5991c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = thread_config(3)\n",
    "\n",
    "#updates\n",
    "for chunk in graph.stream(message_builder(\"user\", \"My name is Nitin and i like video games\"), config, stream_mode=\"updates\"):\n",
    "    # your code here \n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#values\n",
    "for chunk in graph.stream(message_builder(\"user\", \"My name is Nitin and i like video games\"), config=config, stream_mode=\"values\"):\n",
    "    # your code here \n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c714e",
   "metadata": {},
   "source": [
    "# Async mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = thread_config(4)\n",
    "\n",
    "#updates\n",
    "async for event in graph.astream_events(message_builder(\"user\", \"My name is Nitin and i like video games\"), config, version=\"v2\", stream_mode=\"updates\"):\n",
    "    # your code here \n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21f225",
   "metadata": {},
   "source": [
    "Human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c3ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
